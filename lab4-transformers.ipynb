{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30775,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Deep Learning Lab - 4. Transformers\n\nThis lab is for introducting Transformers using TensorFlow and Keras API.\n\n**Transformers** are a type of deep learning model introduced in the paper [\"Attention Is All You Need\"](https://arxiv.org/abs/1706.03762) by Vaswani et al. in 2017. They revolutionized natural language processing (NLP) tasks by using self-attention mechanisms to capture dependencies between different parts of a sentence efficiently.\n\n**Key Concepts:**\n* **Self-Attention:** Helps the model focus on different parts of the input sequence while processing it.\n\n* **Multi-Head Attention:** Multiple attention mechanisms run in parallel to focus on different parts of the sentence at once.\n\n* **Positional Encoding:** Since transformers don’t have a built-in notion of word order (like RNNs), positional encodings are added to input embeddings to give the model information about the order of words.\n\n**Components**\n\n* **Encoder and Decoder:** The transformer consists of an encoder that processes input data and a decoder that generates output data. The encoder processes the input sentence, and the decoder uses the encoder’s representation to predict the output sentence.\n\n* **Stacked Layers:** Multiple encoder and decoder layers are stacked on top of each other to build deeper models.\n\n* **Feed-Forward Networks:** Each encoder and decoder layer has a fully connected feed-forward neural network after the multi-head attention.\n\n<div style=\"text-align: center;\">\n<img src=\"https://machinelearningmastery.com/wp-content/uploads/2021/10/transformer_1.png\" alt=\"Image description\" width=\"300\" height=\"200\">\n</div>\n\nOne of the key innovations of transformers is their ability to process entire sequences of data in parallel, instead of relying on a recurrent approach that processes data one step at a time. This is achieved through the use of self-attention mechanisms, which allow the model to selectively focus on different parts of the input sequence as it processes the data.\n\nIn this tutorial, we will provide a detailed overview of transformers, starting with an explanation of how they work and what makes them unique. We will then walk through a step-by-step guide on how to implement a transformer model in Python, using the TensorFlow deep learning library.","metadata":{}},{"cell_type":"markdown","source":"## Implementation\n\n### Step 0: Install dependencies\nIn this lab, we use [TensorFlow](https://www.tensorflow.org/). TensorFlow is an open-source platform developed by Google for machine learning and deep learning. if you are using Kaggle or Google Colab environment, you can use TensorFlow without installing that but if you want to run your code locally you can install Tensorflow by using the following command with pip:","metadata":{}},{"cell_type":"code","source":"! pip install tensorflow==2.16.1","metadata":{"execution":{"iopub.status.busy":"2024-09-24T21:09:03.225253Z","iopub.execute_input":"2024-09-24T21:09:03.225738Z","iopub.status.idle":"2024-09-24T21:09:03.249644Z","shell.execute_reply.started":"2024-09-24T21:09:03.225691Z","shell.execute_reply":"2024-09-24T21:09:03.248537Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nprint(tf.version.VERSION)","metadata":{"execution":{"iopub.status.busy":"2024-09-24T21:09:03.251903Z","iopub.execute_input":"2024-09-24T21:09:03.252334Z","iopub.status.idle":"2024-09-24T21:09:17.687869Z","shell.execute_reply.started":"2024-09-24T21:09:03.252285Z","shell.execute_reply":"2024-09-24T21:09:17.686740Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"2.16.1\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np","metadata":{"execution":{"iopub.status.busy":"2024-09-24T21:09:17.689498Z","iopub.execute_input":"2024-09-24T21:09:17.690308Z","iopub.status.idle":"2024-09-24T21:09:17.699604Z","shell.execute_reply.started":"2024-09-24T21:09:17.690250Z","shell.execute_reply":"2024-09-24T21:09:17.698365Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"### Step 1: Build Multi Head Attension Sub Layer\n\n","metadata":{}},{"cell_type":"markdown","source":"<div style=\"text-align: center;\">\n<img src=\"https://raw.githubusercontent.com/amanchadha/coursera-deep-learning-specialization/0242d1ffe79086d97b0f210f9664c84ac564abd1/C5%20-%20Sequence%20Models/Week%204/Transformer%20Network/self-attention.png\" alt=\"Image description\" width=\"500\" height=\"300\">\n</div>\nThe scaled dot product attention function computes the attention weight for a sequence of queries (q), keys (k), and values (v). The attention weight measures how much focus should be given to each element in the sequence of values based on the corresponding element in the sequence of queries and keys. The function first computes the dot product between the query and key vectors, then scales the attention logits by dividing them by the square root of the depth of the key vectors. It then applies an optional mask to the attention logits and applies a softmax function to obtain the attention weights. Finally, it computes the weighted sum of the value vectors using the attention weights. The function returns the output and attention weights Attention can be represented by the following equation:\n\n$$\\text { Attention }(Q, K, V)=\\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d_{k}}}+{M}\\right) V\\$$","metadata":{}},{"cell_type":"code","source":"def scaled_dot_product_attention(Q, K, V, mask=None):\n    \n    matmul_QK = tf.matmul(Q,K,transpose_b=True)  # dot-product of shape (..., Tq, Tv)\n\n    dk = K.shape[-1]\n    scaled_attention_logits = matmul_QK/np.sqrt(dk) # scaled dot-product of shape (..., Tq, Tv)\n\n    if mask is not None: \n        scaled_attention_logits += (1. - mask) *(-1e9)\n\n    # Compute the Softmax\n    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # weights of shape (..., Tq, Tv)\n\n    #Multiply with V\n    output = tf.matmul(attention_weights,V)  # Attention representation of shape (..., Tq, dv)\n    \n    return output, attention_weights","metadata":{"execution":{"iopub.status.busy":"2024-09-24T21:09:17.701457Z","iopub.execute_input":"2024-09-24T21:09:17.701965Z","iopub.status.idle":"2024-09-24T21:09:17.738493Z","shell.execute_reply.started":"2024-09-24T21:09:17.701907Z","shell.execute_reply":"2024-09-24T21:09:17.737078Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## MultiHeadAttention\nThe scaled dot product attention is a powerful mechanism that enables a neural network to selectively focus on relevant parts of a sequence when performing tasks such as language translation or image captioning. However, in many cases, there may be multiple aspects or features that an input element wants to attend to, and a single weighted average is not a sufficient way to capture this information. To address this, we can extend the attention mechanism to multiple heads.\n\n<div style=\"text-align: center;\">\n<img src=\"https://www.researchgate.net/publication/380427185/figure/fig3/AS:11431281241719045@1715204009536/The-overall-structure-of-multi-head-attention.jpg\" alt=\"Image description\" width=\"300\" height=\"200\">\n</div>\n\nThe idea behind multi-head attention is to perform several different attention operations in parallel, with each attention head focusing on a different aspect of the input sequence. In other words, instead of a single query-key-value triplet, we use multiple such triplets in parallel, each one focusing on a different aspect of the input sequence.\n\nTo achieve this, we first split the query, key, and value matrices into several submatrices, each of which represents a different aspect of the input sequence. Then, for each submatrix, we apply the scaled dot product attention mechanism independently. This results in several different attention outputs, each one representing a different aspect of the input sequence.\n\nFinally, we concatenate the attention outputs and apply a linear transformation using a weight matrix to obtain a combined output. This combined output represents the final attention output that captures all the relevant aspects of the input sequence.\n\nMathematically, we can express the multi-head attention operation as a matrix operation involving the query, key, and value matrices, as well as a set of learnable weight matrices used to combine the attention outputs.","metadata":{}},{"cell_type":"code","source":"class Multihead_Attention(tf.keras.layers.Layer):\n    def __init__(self, H, d_model, dk, dv):  \n        super(Multihead_Attention, self).__init__()\n        \n        initializer = tf.keras.initializers.GlorotUniform()\n        self.WQ = tf.Variable(initializer(shape=(H, d_model, dk)), trainable=True)\n        self.WK = tf.Variable(initializer(shape=(H, d_model, dk)), trainable=True)\n        self.WV = tf.Variable(initializer(shape=(H, d_model, dv)), trainable=True)\n        self.WO = tf.Variable(initializer(shape=(H*dv,d_model)), trainable=True)\n\n    \n    def call(self, Q, K, V, mask=None):\n        #Projecting Q,K,V to Qh, Kh, Vh. The H projection are stacked on the penultiem axis\n        Qh= tf.experimental.numpy.dot(Q, self.WQ) #of shape (batch_size, Tq, H, dk)\n        Kh= tf.experimental.numpy.dot(K, self.WK) #of shape (batch_size, Tv, H, dk)\n        Vh= tf.experimental.numpy.dot(V, self.WV) #of shape (batch_size, Tv, H, dv)\n        \n        #Transposition\n        Qh=tf.transpose(Qh, [0,2,1,3]) #of shape (batch_size, H, Tq, dk)\n        Kh=tf.transpose(Kh, [0,2,1,3]) #of shape (batch_size, H, Tv, dk)\n        Vh=tf.transpose(Vh, [0,2,1,3]) #of shape (batch_size, H, Tv, dv)\n        \n        # Computing the dot-product attention\n        Ah,_=scaled_dot_product_attention(Qh, Kh, Vh, mask=mask) #of shape (batch_size, H, Tq, dv)\n        \n        #Flattening the H and dv axis and projecting back to d_model\n#        A = tf.reshape(Ah,(*Ah.shape[:-2],-1))\n        s=Ah.shape\n        A = tf.reshape(Ah,(s[0],s[2],s[1]*s[3])) #of shape (batch_size, Tq, H*dv)\n        A= tf.experimental.numpy.dot(A, self.WO) #of shape (batch_size, Tq, d_model)\n        \n        return A","metadata":{"execution":{"iopub.status.busy":"2024-09-24T21:09:17.742366Z","iopub.execute_input":"2024-09-24T21:09:17.743071Z","iopub.status.idle":"2024-09-24T21:09:17.833281Z","shell.execute_reply.started":"2024-09-24T21:09:17.743027Z","shell.execute_reply":"2024-09-24T21:09:17.832272Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"### Step 2: Build Feed Forward Neural Network Sub Layer","metadata":{}},{"cell_type":"code","source":"class FNNLayer(tf.keras.layers.Layer):\n    def __init__(self, d_model, dff):\n        super(FNNLayer, self).__init__()\n\n        self.layer1 = tf.keras.layers.Conv1D(filters=dff, kernel_size=1,activation=\"relu\")\n        self.layer2 = tf.keras.layers.Conv1D(filters=d_model, kernel_size=1)\n\n\n    def call(self, x):\n        x=self.layer1(x)\n        fnn_layer_out=self.layer2(x)\n \n        return fnn_layer_out","metadata":{"execution":{"iopub.status.busy":"2024-09-24T21:09:17.834648Z","iopub.execute_input":"2024-09-24T21:09:17.835010Z","iopub.status.idle":"2024-09-24T21:09:17.842939Z","shell.execute_reply.started":"2024-09-24T21:09:17.834972Z","shell.execute_reply":"2024-09-24T21:09:17.841648Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"### Step 3: Implement Positional Encoding","metadata":{}},{"cell_type":"markdown","source":"When working with sequence to sequence tasks, the order of the data is crucial. While training RNNs, the input order is preserved automatically. However, when training Transformer networks, all data is input at once, and there's no inherent order information. To overcome this, positional encoding is used to specify the position of each input in the sequence. This encoding is achieved through sine and cosine formulas as follows:\n\n$$\nPE_{(pos, 2i)}= sin\\left(\\frac{pos}{{10000}^{\\frac{2i}{d}}}\\right)\n\\tag{1}$$\n\n<br>\n$$PE_{(pos, 2i+1)}= cos\\left(\\frac{pos}{{10000}^{\\frac{2i}{d}}}\\right)\n\\tag{2}$$\n\nHere, $pos$ refers to the position of the input in the sequence, $i$ refers to the index of the dimension in the embedding vector, and $d$ refers to the dimensionality of the model.","metadata":{}},{"cell_type":"code","source":"def positional_encoding(positions, d):\n    # initialize a matrix angle_rads of all the angles\n    pos=np.arange(positions)[:, np.newaxis] #Column vector containing the position span [0,1,..., positions]\n    k= np.arange(d)[np.newaxis, :]  #Row vector containing the dimension span [[0, 1, ..., d-1]]\n    i = k//2\n    angle_rads = pos/(10000**(2*i/d)) #Matrix of angles indexed by (pos,i)\n    \n    # apply sin to even indices in the array; 2i\n    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n  \n    # apply cos to odd indices in the array; 2i+1\n    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n\n    #adds batch axis\n    pos_encoding = angle_rads[np.newaxis, ...] \n    \n    return tf.cast(pos_encoding, dtype=tf.float32)","metadata":{"execution":{"iopub.status.busy":"2024-09-24T21:09:17.844784Z","iopub.execute_input":"2024-09-24T21:09:17.845311Z","iopub.status.idle":"2024-09-24T21:09:17.861691Z","shell.execute_reply.started":"2024-09-24T21:09:17.845260Z","shell.execute_reply":"2024-09-24T21:09:17.860521Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"### Step 4: Build Encoder","metadata":{}},{"cell_type":"markdown","source":"\nThe Encoder consists of a stack of identical layers, where each layer has two sub-layers: a multi-head self-attention mechanism and a position-wise fully connected feedforward network.\n\n<div style=\"text-align: center;\">\n<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSyz2j58Hk1uFj_b_sQh6ecy3gP4bGMAgs1_q4zPivrcAkfp4kd8XEqTOQn0TQYdpjJ1vU&usqp=CAU\" alt=\"Image description\" width=\"200\" height=\"200\">\n</div>\n\n\nIn the self-attention mechanism, the Encoder attends to all positions of the input sequence to compute a weighted sum of the values at each position, where the weights are determined by the similarity between the query and key vectors for each position. This allows the Encoder to capture dependencies between all positions of the input sequence in parallel, and to assign more weight to the positions that are most relevant to the current position.\n\nThe fully connected feedforward network consists of two linear transformations with a ReLU activation function in between, which is applied to each position in the sequence independently. This allows the Encoder to learn non-linear relationships between the hidden states at different positions of the sequence.\n\nIn addition to the sub-layers, each layer in the Encoder also has residual connections and layer normalization, which help to mitigate the vanishing gradient problem and improve training stability.\n\nThe residual connection allows the output of the sub-layer to be added to the original input sequence, which preserves information from the input sequence and helps to propagate gradients through the network. The layer normalization normalizes the output of the sub-layer with respect to the mean and variance of the hidden states, which helps to reduce the effects of covariate shift and improve convergence.\n\nThe output of the Encoder is a sequence of hidden states that contains information about the context of each input token. This sequence is then passed to the Transformer Decoder for further processing, where it is used to generate an output sequence.\n","metadata":{}},{"cell_type":"code","source":"class EncoderLayer(tf.keras.layers.Layer):\n\n    def __init__(self, H, d_model, dk, dv, dff, dropout_rate=0.1, layernorm_eps=1e-6):\n        super(EncoderLayer, self).__init__()\n        \n        self.mha = Multihead_Attention(H, d_model, dk, dv)\n        self.ffn = FNNLayer(d_model, dff)\n        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=layernorm_eps)\n        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=layernorm_eps)\n        self.dropout_mha = tf.keras.layers.Dropout(dropout_rate)\n        self.dropout_ffn = tf.keras.layers.Dropout(dropout_rate)\n    \n    def call(self, x, training=False, mask=None):\n        A = self.mha(x,x,x,mask=mask) # Self attention (batch_size, Tq, d_model)\n        A = self.dropout_mha(A, training=training) #Apply Dropout during training\n        \n        \n        #  Residual connection + Layer normalization\n        out1 = self.layernorm1(x+A)  # (batch_size, Tq, d_model)\n\n        # Pointwise ffn\n        ffn_output = self.ffn(out1) # (batch_size, Tq, d_model)\n        ffn_output = self.dropout_ffn(ffn_output, training=training) # Apply Dropout during training\n        \n        # Residual connection + Layer normalization\n        encoder_layer_out = self.layernorm2(ffn_output+out1)  # (batch_size, input_seq_len, fully_connected_dim)\n        \n        return encoder_layer_out","metadata":{"execution":{"iopub.status.busy":"2024-09-24T21:09:17.863394Z","iopub.execute_input":"2024-09-24T21:09:17.864090Z","iopub.status.idle":"2024-09-24T21:09:17.875963Z","shell.execute_reply.started":"2024-09-24T21:09:17.864004Z","shell.execute_reply":"2024-09-24T21:09:17.874812Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class Encoder(tf.keras.layers.Layer):\n\n    def __init__(self, N, H, d_model, dk, dv, dff, dropout_rate=0.1, layernorm_eps=1e-6):\n        super(Encoder, self).__init__()\n        \n        self.layers=[EncoderLayer(H, d_model, dk, dv, dff, \n                                  dropout_rate=dropout_rate, \n                                  layernorm_eps=layernorm_eps)\n                                  for i in range(N)]\n    \n    def call(self, x, training=False, mask=None):\n        for layer in self.layers:\n            x = layer(x, training=training, mask=mask)\n                                  \n        return x\n                                  ","metadata":{"execution":{"iopub.status.busy":"2024-09-24T21:09:17.877882Z","iopub.execute_input":"2024-09-24T21:09:17.878848Z","iopub.status.idle":"2024-09-24T21:09:17.892884Z","shell.execute_reply.started":"2024-09-24T21:09:17.878795Z","shell.execute_reply":"2024-09-24T21:09:17.891657Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"### Step 5: Build Decoder","metadata":{}},{"cell_type":"markdown","source":"The decoder takes in the encoded input sequence along with the previous generated output sequence. The output sequence is first passed through an embedding layer, which maps each token to a high-dimensional vector space. The embedding output is then added with a positional encoding, which allows the model to encode the sequential order of the input/output sequence. The positional encoding is added to the embeddings through a simple addition operation.\n\n<div style=\"text-align: center;\">\n<img src=\"https://encrypted-tbn1.gstatic.com/images?q=tbn:ANd9GcRuKZ3AKs-84k2hE7vxgQW8tFgk2w7fX2o5vyl0-HmGvdXjoAiH\" alt=\"Image description\" width=\"200\" height=\"200\">\n</div>\n\nNext, the decoder applies a multi-head self-attention mechanism similar to that of the encoder. However, the decoder also uses an additional masked self-attention mechanism, which prevents the decoder from attending to future tokens in the output sequence during training. This is because during training, the decoder is not yet aware of the future tokens and hence should not attend to them. During inference, the masked self-attention mechanism is not used as the model is generating the output sequence token by token, and hence it does not have access to the future tokens.\n\nThe decoder then applies a feedforward neural network to each position in the sequence. The output of the feedforward neural network is passed through a residual connection, followed by layer normalization. The residual connection allows the model to learn the difference between the input and output of the layer, while layer normalization helps to stabilize the training process.\n\nThe output of the decoder is then passed through a linear layer, which maps the high-dimensional vector space to the output vocabulary size. A softmax activation function is applied to the output to obtain the final probability distribution over the output vocabulary. The model then samples the token from this distribution and repeats the process until an end-of-sequence token is generated.","metadata":{}},{"cell_type":"code","source":"class DecoderLayer(tf.keras.layers.Layer):\n\n    def __init__(self, H, d_model, dk, dv, dff, dropout_rate=0.1, layernorm_eps=1e-6):\n        super(DecoderLayer, self).__init__()\n        \n        self.mha1 = Multihead_Attention(H, d_model, dk, dv)\n        self.mha2 = Multihead_Attention(H, d_model, dk, dv)\n        self.ffn = FNNLayer(d_model, dff)\n        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=layernorm_eps)\n        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=layernorm_eps)\n        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=layernorm_eps)\n        self.dropout_mha1 = tf.keras.layers.Dropout(dropout_rate)\n        self.dropout_mha2 = tf.keras.layers.Dropout(dropout_rate)                                     \n        self.dropout_ffn = tf.keras.layers.Dropout(dropout_rate)\n    \n    def call(self, x, encoder_output, training=False, look_ahead_mask=None, padding_mask=None):\n        # 1st Masked MultiHead attention                                     \n        A1 = self.mha1(x,x,x,mask=look_ahead_mask) # Self attention (batch_size, Tq, d_model)\n        A1 = self.dropout_mha1(A1, training=training) #Apply Dropout during training\n        \n        #  Residual connection + Layer normalization\n        out1 = self.layernorm1(x+A1)  # (batch_size, Tq, d_model)\n\n        # 2nd Masked MultiHead attention                                     \n        A2 = self.mha2(x,encoder_output,encoder_output,mask=padding_mask) # Self attention (batch_size, Tq, d_model)\n        A2 = self.dropout_mha2(A2, training=training) #Apply Dropout during training\n        \n        \n        #  Residual connection + Layer normalization\n        out2 = self.layernorm2(out1+A2)  # (batch_size, Tq, d_model)\n                                             \n        # Pointwise ffn\n        ffn_output = self.ffn(out2) # (batch_size, Tq, d_model)\n        ffn_output = self.dropout_ffn(ffn_output, training=training) # Apply Dropout during training\n        \n        # Residual connection + Layer normalization\n        decoder_layer_out = self.layernorm3(ffn_output+out2)  # (batch_size, input_seq_len, fully_connected_dim)\n        \n        return decoder_layer_out","metadata":{"execution":{"iopub.status.busy":"2024-09-24T21:09:17.894728Z","iopub.execute_input":"2024-09-24T21:09:17.895165Z","iopub.status.idle":"2024-09-24T21:09:17.909006Z","shell.execute_reply.started":"2024-09-24T21:09:17.895123Z","shell.execute_reply":"2024-09-24T21:09:17.907805Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"class Decoder(tf.keras.layers.Layer):\n\n    def __init__(self, N, H, d_model, dk, dv, dff, dropout_rate=0.1, layernorm_eps=1e-6):\n        super(Decoder, self).__init__()\n        \n        self.layers=[DecoderLayer(H, d_model, dk, dv, dff, \n                                  dropout_rate=dropout_rate, \n                                  layernorm_eps=layernorm_eps)\n                                  for i in range(N)]\n    \n    def call(self, x, encoder_output, training=False, look_ahead_mask=None, padding_mask=None):\n        for layer in self.layers:\n            x = layer(x,encoder_output, look_ahead_mask=look_ahead_mask, padding_mask=padding_mask)\n                                  \n        return x","metadata":{"execution":{"iopub.status.busy":"2024-09-24T21:09:17.910514Z","iopub.execute_input":"2024-09-24T21:09:17.910868Z","iopub.status.idle":"2024-09-24T21:09:17.925959Z","shell.execute_reply.started":"2024-09-24T21:09:17.910831Z","shell.execute_reply":"2024-09-24T21:09:17.924442Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"### Step 6: Define the Model","metadata":{}},{"cell_type":"markdown","source":"The Transformer is composed of two main components: the encoder and the decoder. The encoder takes an input sequence and produces a sequence of hidden representations, while the decoder takes this sequence of hidden representations and generates an output sequence. Both the encoder and decoder are composed of several layers of multi-headed self-attention and point-wise feed-forward neural networks.\n\n<div style=\"text-align: center;\">\n<img src=\"https://machinelearningmastery.com/wp-content/uploads/2021/10/transformer_1.png\" alt=\"Image description\" width=\"300\" height=\"200\">\n</div>\n","metadata":{}},{"cell_type":"code","source":"class Transformer(tf.keras.Model):\n    \n    def __init__(self, N, H, d_model, dk, dv, dff, \n                 vocab_size, max_positional_encoding, \n                 dropout_rate=0.1, layernorm_eps=1e-6):\n\n        super(Transformer, self).__init__()\n        \n        initializer = tf.keras.initializers.GlorotUniform()\n        self.embedding = tf.Variable(initializer(shape=(vocab_size, d_model)), trainable=True)\n        self.PE = positional_encoding(max_positional_encoding, d_model)\n        \n        self.dropout_encoding_input = tf.keras.layers.Dropout(dropout_rate)\n        self.dropout_decoding_input = tf.keras.layers.Dropout(dropout_rate)\n        \n        self.encoder = Encoder(N, H, d_model, dk, dv, dff, dropout_rate=dropout_rate, layernorm_eps=layernorm_eps)\n        self.decoder = Decoder(N, H, d_model, dk, dv, dff, dropout_rate=dropout_rate, layernorm_eps=layernorm_eps)\n\n        \n\n    def call(self, x, y, training=False, enc_padding_mask=None, look_ahead_mask=None, dec_padding_mask=None):\n        \n        x = tf.matmul(x,self.embedding)\n        x = x + self.PE\n        x =  self.dropout_encoding_input(x,training=training)\n        \n        encoder_output = self.encoder(x,training=training, mask=enc_padding_mask)\n        \n        y = tf.matmul(y,self.embedding)\n        y = y + self.PE\n        y = self.dropout_decoding_input(y,training=training)\n        \n        dec_output = self.decoder(y, encoder_output, training=training, \n                                  look_ahead_mask=look_ahead_mask, padding_mask=dec_padding_mask)\n        \n        \n        pred =  tf.matmul(self.embedding,dec_output,transpose_b=True)\n        pred = tf.nn.softmax(pred)\n        \n        return pred","metadata":{"execution":{"iopub.status.busy":"2024-09-24T21:09:17.927399Z","iopub.execute_input":"2024-09-24T21:09:17.927759Z","iopub.status.idle":"2024-09-24T21:09:17.941800Z","shell.execute_reply.started":"2024-09-24T21:09:17.927722Z","shell.execute_reply":"2024-09-24T21:09:17.940713Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"N, H, d_model, dk, dv, dff = 6, 8, 512, 64, 64, 2048\nvocab_size, T =29, 11\nbatch_size = 3\n\n\ntransformer = Transformer(N, H, d_model, dk, dv, dff, \n                 vocab_size, T)\n\ninput_shape = (None, T,vocab_size)\n\n\nx = tf.random.uniform((batch_size, T, vocab_size))\ny =  tf.random.uniform((batch_size, T, vocab_size))\n\npred = transformer(x,y,training=True)\nprint(pred.shape)\n\ntransformer.summary()","metadata":{"execution":{"iopub.status.busy":"2024-09-24T21:09:17.943301Z","iopub.execute_input":"2024-09-24T21:09:17.943730Z","iopub.status.idle":"2024-09-24T21:09:26.041511Z","shell.execute_reply.started":"2024-09-24T21:09:17.943688Z","shell.execute_reply":"2024-09-24T21:09:26.040211Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"(3, 29, 11)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"transformer\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"transformer\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ encoder (\u001b[38;5;33mEncoder\u001b[0m)               │ ?                      │    \u001b[38;5;34m12,610,560\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ decoder (\u001b[38;5;33mDecoder\u001b[0m)               │ ?                      │    \u001b[38;5;34m12,616,704\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ encoder (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Encoder</span>)               │ ?                      │    <span style=\"color: #00af00; text-decoration-color: #00af00\">12,610,560</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ decoder (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Decoder</span>)               │ ?                      │    <span style=\"color: #00af00; text-decoration-color: #00af00\">12,616,704</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m25,227,264\u001b[0m (96.23 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,227,264</span> (96.23 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m25,227,264\u001b[0m (96.23 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,227,264</span> (96.23 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}]},{"cell_type":"code","source":"optimizer = tf.keras.optimizers.Adam(learning_rate=0.1, epsilon=1e-09)\n\ntransformer.compile(loss='crossentropy',optimizer=optimizer,metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2024-09-24T21:10:15.430520Z","iopub.execute_input":"2024-09-24T21:10:15.430964Z","iopub.status.idle":"2024-09-24T21:10:15.448506Z","shell.execute_reply.started":"2024-09-24T21:10:15.430924Z","shell.execute_reply":"2024-09-24T21:10:15.447167Z"},"trusted":true},"execution_count":14,"outputs":[]}]}